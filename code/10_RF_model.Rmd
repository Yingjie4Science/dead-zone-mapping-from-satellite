---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Set up

```{r Packages}
# To clear your environment
# remove(list = ls())

library(randomForest)
library(tidyverse)
require(hydroGOF)    ## for calculating RMSE and MAE


```


```{r Dirs}
### Set work dir ----------------------------------------------------------
path <- rstudioapi::getSourceEditorContext()$path
dir  <- dirname(rstudioapi::getSourceEditorContext()$path)
dir
dirname(dir)        ## go to parent dir
setwd(dirname(dir)) ## set this parent dir as root dir
getwd()

### the data fir
dir.band    <- './data/data_from_gee/'
dir.fig     <- paste0(dirname(dir), '/figures/')
dir.fig
dir.cleaned <- paste0(dir.band, 'Img2Table_cleaned/')
dir.cleaned

## keep more decimals
options(digits = 15)
options(pillar.sigfig = 15)

### Disable Scientific Notation in R
options(scipen = 999) # Modify global options in R
```



```{r Functions}
#' Detect outliers using IQR method
#' 
#' @param x A numeric vector
#' @param na.rm Whether to exclude NAs when computing quantiles
#' 
is_outlier <- function(x, na.rm = T) {
  qs = quantile(x, probs = c(0.25, 0.75), na.rm = na.rm)

  lowerq <- qs[1]
  upperq <- qs[2]
  iqr = upperq - lowerq 

  extreme.threshold.upper = (iqr * 3) + upperq
  extreme.threshold.lower = lowerq - (iqr * 3)

  # Return logical vector
  x > extreme.threshold.upper | x < extreme.threshold.lower
}

#' Remove rows with outliers in given columns
#' 
#' Any row with at least 1 outlier will be removed
#' 
#' @param df A data.frame
#' @param cols Names of the columns of interest. Defaults to all columns.
#' 
#' 
remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    cat("Removing outliers in column: ", col, " \n")
    df <- df[!is_outlier(df[[col]]),]
  }
  df
}



func_accuracy_metrics <- function(predicted, actual){
  value_rmse <- hydroGOF::rmse(sim = predicted, obs = actual, na.rm=TRUE)
  value_mae  <- hydroGOF::mae(sim  = predicted, obs = actual, na.rm=TRUE)
  
  df <- data.frame(predicted = predicted, actual = actual)
  r2 <- summary(lm(actual ~ predicted, data=df))$r.squared
  
  print(paste0('RMSE: ', round(value_rmse, digits = 4)))
  print(paste0('MAE:  ', round(value_mae,  digits = 4)))
  print(paste0('R2:  ',  round(r2,         digits = 4)))
  
  error <- data.frame(RMSE = value_rmse, MAE = value_mae, R2 = r2)
  return(error)
}
```





# Data

```{r}
# Data ---------------------------------------------------------------------------------------------
# fname <- paste0(dir.cleaned, 'by_timelag_withDO/sample_2005to2019_pixelValue_withDOmin_1_weekBefore.xlsx');    fname
fname <- paste0(dir.cleaned, 'by_timelag_withDO/sample_2005to2019_pixelValue_withDObottom_1_weekBefore.xlsx'); fname

df <- readxl::read_excel(path = fname) %>%
  dplyr::filter(year == 2014) %>%
  # dplyr::filter(year >= 2014 & year <= 2015) %>%
  rename(station = YEID, oxmgl = DO)


### aggregate by the mean value of all the bands for each station
xmean <- aggregate(df[, 4:17], by=list(df$station), FUN=mean, na.rm=T)
### aggregate by the mean value of the ox data for each station
ymean <- aggregate(df$oxmgl, by=list(df$station), FUN=mean, na.rm=T)

xvar <- xmean[,-1] ## remove the column of station id 
yvar <- ymean[,-1]

rf.data <- cbind(yvar, xvar) ## put y and x(s) in one table 
str(rf.data)
rf.data.na.omit <- na.omit(rf.data)

## >> to choose input data ---------------------------------
data <- rf.data
# data <- rf.data.na.omit

```



## RF 1
```{r}

# 70% date for training, 30% for testing
set.seed(123)
train <- sample(nrow(data), nrow(data)*0.7)
data_train <- data[train, ]
data_test  <- data[-train, ]



# RF model -----------------------------------------------------------------------------------------
## run rf model 
n_tree <- 100 ## 99
rf.model <- randomForest(yvar ~ ., data = data_train, ntree = n_tree, 
                         importance = TRUE, norm.votes = TRUE, proximity = TRUE, 
                         # mtry  = 8, ## --> need to use RF tune process to decide
                         na.action = na.omit) 
## na.action = na.omit
## na.action = na.roughfix  --> Impute Missing Values by median/mode.

rf.model
attributes(rf.model)

plot(rf.model)

# 1. accuracy using training dataset 
predict_train <- predict(rf.model, data_train)
plot(data_train$yvar, predict_train, main = 'Training sample', xlab = 'obs', ylab = 'Predict')
abline(0, 1) ## abline(0,1) will draw a 1:1 line


# 2. accuracy using testing dataset 
predict_test <- predict(rf.model, data_test)
plot(data_test$yvar, predict_test, main = 'Testing sample', xlab = 'obs', ylab = 'Predict')
abline(0, 1)

# Calculating RMSE using rmse()         
func_accuracy_metrics(predicted = predict_train, actual = data_train$yvar)
func_accuracy_metrics(predicted = predict_test,  actual = data_test$yvar)
```




```{r plot obs-predict}
library(ggpmisc)
my.formula <- y ~ x

df <- data.frame(predicted = predict_train, actual = data_train$yvar); which = 'train'
# df <- data.frame(predicted = predict_test,  actual = data_test$yvar);  which = 'test'

### plot for `predicted` vs. `actual`
cor <- df  %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.7, shape = 16) +
  geom_smooth(method = 'lm', ## lm
              na.rm = T,
              formula = my.formula) +
  stat_poly_eq(formula = my.formula, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), parse = TRUE) +
  geom_abline(linetype="dashed",size=1, color = 'red') +
  scale_x_continuous(breaks = seq(0, 8, by = 2), limits = c(0, 8)) +
  scale_y_continuous(breaks = seq(0, 8, by = 2), limits = c(0, 8))+
  theme_bw() 
cor
fname <- paste0(dir.fig, 'corr_obs_predict_', which, '.png'); fname
# ggsave(filename = fname, plot = cor, width = 3.2, height = 3.2, units = 'in', dpi = 300)

```




  Loop time lags and to see what time lag predicts the best. 
```{r loop}

### -> 1. to choose which year(s)
### -> 2. to choose weekly data OR daily data
### -> 3. to choose which DO to use (DObottom, Do10m)

### ---> to choose: 

# y1 <- 2014; y2 <- 2014
y1 <- 2005; y2 <- 2019

time_unit = 'Week'
# time_unit = 'Day'

whichDO <- 'DObottom'
# whichDO <- 'Do10m'




pat <- paste0('^sample_2005to2019_pixelValue_with', whichDO)
f.list <- list.files(path = paste0(dir.cleaned, 'by_timelag_withDO/'), pattern = pat, full.names = T); f.list
f.list <- f.list[grepl(x = f.list, time_unit, ignore.case = T)]; f.list

accuracy_ls <- data.frame()

for (f in f.list) {
  print(f)
  
  for (yr in seq(y1, y2)) {
    print(yr)
    
    
    data <- readxl::read_excel(path = f) %>%
      dplyr::filter(year == yr) %>%
      rename(station = YEID, oxmgl = DO) %>%
      dplyr::select(oxmgl, chlor_a:sst) %>%
      rename(yvar = oxmgl)
    
    nw <- unique(readxl::read_excel(path = f)$nweek_before); print(nw)
  
    ### RF model
    # 70% date for training, 30% for testing
    set.seed(123)
    train <- sample(nrow(data), nrow(data)*0.7)
    data_train <- data[train, ]
    data_test  <- data[-train, ]
    
    rf.model <- randomForest(yvar ~ ., data = data_train, ntree = n_tree, 
                             importance = TRUE, norm.votes = TRUE, proximity = TRUE, 
                             # mtry  = 8, ## --> need to use RF tune process to decide
                             na.action = na.omit) 
    
    ### accuracy
    predict_train <- predict(rf.model, data_train)
    accuracy <- func_accuracy_metrics(predicted = predict_train, actual = data_train$yvar)
    accuracy <- cbind(year = yr, nw = nw, accuracy)
    accuracy_ls <- rbind(accuracy_ls, accuracy)
    
  }
}


```




```{r plot}
### plot
acc <- accuracy_ls %>%
  gather(key = err, value = value, RMSE:R2) %>%
  dplyr::mutate(err = factor(err, levels = c('R2', 'RMSE', 'MAE'))) 


theme_my <- 
  theme_bw() +
  theme(legend.title = element_blank(), 
        # panel.grid = element_blank(),
        legend.background = element_rect(fill="transparent"),
        # legend.position = c(0.2, 0.75),
        ) 


### if only choosing 1-2 years data, we plot line graph
### if choosing several  years data, we first plot boxplot and then plot line graph of the mean
if (y2 - y1 < 3) {
  
  p_acc <-
    ggplot(data = acc, aes(x = -nw, y = value, color = err)) +
  
    ### 1. point ---------------
    # geom_point() +
    # geom_smooth(method = 'loess', formula = 'y ~ x') +
    
    ## 2. line ----------------
    geom_line() +
    geom_vline(xintercept = with(acc, -nw[which.max(value)]),
               linetype="dashed",size = .5) +
    geom_vline(xintercept = with(acc %>% filter(err != 'R2'), -nw[which.min(value)]),
               linetype="dashed",size = .5) +
    theme_my +
    # scale_x_continuous(breaks = seq(-max(acc$nw), 0, by = 3)) +
    xlab(paste0(time_unit, '')) +
    ggtitle(paste(y1, y2, whichDO, sep = '-'))

  
} else {
  ### plot box plot ---
  p_acc <-
    ggplot(data = acc, aes(x = factor(-nw), y = value, fill = err)) +
    geom_boxplot() +
    theme_my +
    ggtitle(paste(y1, y2, whichDO, sep = '-'))
  
  
  ### and plot the line of mean ---
  ### by time lag and by year --> calculate the mean `error` of multiple years at each time lag
  acc_mean <- accuracy_ls %>%
    gather(key = err, value = value, RMSE:R2) %>%
    dplyr::mutate(err = factor(err, levels = c('R2', 'RMSE', 'MAE'))) %>%
    ungroup() %>%
    group_by(nw, err) %>%
    summarise(value = mean(value, rm.na = T))
  
  p_acc_mean <-
    ggplot(data = acc_mean, aes(x = -nw, y = value, color = err)) +
    geom_line() +
    geom_vline(xintercept = with(acc_mean, -nw[which.max(value)]),
               linetype="dashed",size = .5) +
    geom_vline(xintercept = with(acc_mean %>% filter(err != 'R2'), -nw[which.min(value)]),
               linetype="dashed",size = .5) +
  
    theme_my +
    scale_x_continuous(breaks = seq(-max(acc_mean$nw), 0, by = 3)) +
    xlab(paste0(time_unit, ''))+
    ggtitle(paste(y1, y2, whichDO, sep = '-'))
  p_acc_mean
  fname <- paste0(dir.fig, 'accuracy_rf_by', paste(time_unit, whichDO, y1, y2, sep = '_'), '_Mean.png'); fname
  ggsave(filename = fname, plot = p_acc_mean, width = 5, height = 3.2, units = 'in', dpi = 300)

}


p_acc
fname <- paste0(dir.fig, 'accuracy_rf_by', paste(time_unit, whichDO, y1, y2, sep = '_'), '.png'); fname
ggsave(filename = fname, plot = p_acc, width = 5, height = 3.2, units = 'in', dpi = 300)
```




## RF 2 - tune vars
```{r}

### Variable Importance #############
importance <- rf.model$importance %>% as.data.frame()
head(importance, 10)

# or using the function -> importance()
# importance <- data.frame(importance(rf.model), check.names = T)
# head(importance, 10)

# plot the top 10 important vars 
varImpPlot(rf.model, sort = T, n.var = min(10, nrow(rf.model$importance)), 
           main = 'Top 10 - variable importance')

# rank the importance list by choosing one indicator, e.g., 'IncNodePurity' 
importance <- importance[order(importance$IncNodePurity, decreasing = TRUE), ]
importance

# save to file
#write.table(importance, 'importance.txt', sep = '\t', col.names = NA, quote = FALSE)




## Random Forest Cross-Valdidation for feature selection  =====================
## 交叉验证辅助评估选择特定数量的 OTU, # 5 次重复十折交叉验证
## Not sure how to use the code.... this is to help decide how many variables should be included in the model.
## given I see the number is usually 9 - 13, I will decide to choose 10
# set.seed(123)
# data_train.cv <- replicate(5, rfcv(trainx = subset(data_train, select = -yvar), 
#                                    trainy = data_train$yvar, 
#                                    cv.fold = 3, ## 10 
#                                    step = .5), 
#                            simplify = FALSE)
# data_train.cv




# select the top n most important variables 
n <- 10
importance_var_selected <- importance[1:n, ]
importance_var_selected

var.select <- rownames(importance_var_selected); var.select
data.select <- data[, c(var.select, 'yvar')]
data.select <- reshape2::melt(data.select, id = 'yvar')


ggplot(data.select, aes(x = yvar, y = value)) +
  geom_point() +
  geom_smooth(formula = y~x, method = 'loess') +
  facet_wrap(~variable, ncol = n/2, scale = 'free_y') +
  labs(title = '', x = 'DO', y = 'Var') +
  theme_bw()



# training sample 70%, test sample 30%
data.select <- data[, c(var.select, 'yvar')]
set.seed(123)
train <- sample(nrow(data.select), nrow(data.select)*0.7)
data_train <- data.select[train, ]
data_test  <- data.select[-train, ]


# RF model
set.seed(123)
rf.model.selectVar <- randomForest(yvar~., data = data_train, ntree = n_tree,
                                   importance = TRUE, norm.votes = TRUE, proximity = TRUE, 
                                   na.action = na.omit)
rf.model.selectVar

# 1. accuracy using training dataset 
predict_train <- predict(rf.model.selectVar, data_train)
plot(data_train$yvar, predict_train, main = 'Training sample', xlab = 'actual', ylab = 'predicted')
abline(1, 1)

# 2. accuracy using testing dataset 
predict_test <- predict(rf.model.selectVar, data_test)
plot(data_test$yvar, predict_test, main = 'Testing sample', xlab = 'actual', ylab = 'predicted')

abline(1, 1)


# Calculating RMSE using rmse()         
func_accuracy_metrics(predicted = predict_train, actual = data_train$yvar)
func_accuracy_metrics(predicted = predict_test,  actual = data_test$yvar)
```




**RF 3 - tune mtry**
  In process ...

```{r }
## tune RF =========================================================
# tuning with mtry
# - mtry:  Number of variables randomly sampled as candidates at each split.
# - ntree: Number of trees to grow.
dev.off()


## --> input MUST NOT have NA
data_noNA <- na.omit(data)
tune_RF <- tuneRF(x = data_noNA[,-1], y = data_noNA[,1], 
                  # stepFactor = 0.1,           ## at each iteration, mtry is inflated (or deflated) by this value
                  plot = T,                     ## whether to plot the OOB error as function of mtry
                  # doBest = T,
                  ntreeTry = n_tree,            ## number of trees used at the tuning step, because:
                  # according "plot(rf)" 500 trees are not necessary since the error is stable after 100 trees
                  trace=T,                      ## whether to print the progress of the search
                  improve = 0.005)              ## the (relative) improvement in OOB error must be by this much for the search to continue

print(tune_RF)

# mtry with smallest error should be used for train RF
# in this case mtry = 8 is already the best choice

# AFTER TUNING, best choice would be:
rf <- randomForest(yvar ~ ., data  = data, ntree = n_tree, 
                   importance=TRUE, norm.votes=TRUE, proximity=TRUE, 
                   na.action = na.omit,
                   mtry  = 8)

rf
plot(rf)

### histogram: number of nodes in the tree
hist(treesize(rf), main ="number of nodes in the tree")


### Variable Importance
varImpPlot(rf, sort = T, n.var=10,
           main="Top 10 - Variable Importance")


imp <- rf$importance
imp
impvar <- rownames(imp); impvar


### partialPlot ##############################
op <- par(mfrow=c(3, 5))
for (i in seq_along(impvar)) {
  partialPlot(rf, data, impvar[i], xlab=impvar[i],
              main=paste(impvar[i]),
  )
}
par(op)
dev.off()

```




