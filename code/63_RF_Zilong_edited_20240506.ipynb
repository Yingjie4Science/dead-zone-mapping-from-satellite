{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\Shared drives\\gdrive\\_paper\\_phd_dissertation\\ch4_dead_zone\\Dead_Zone_telecoupling\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import *\n",
    "from sklearn.ensemble  import  RandomForestRegressor\n",
    "from sklearn.model_selection  import  train_test_split\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## need to change working directory when using Spyder \n",
    "path_current = os.getcwd()\n",
    "path_root    = os.path.dirname(path_current)\n",
    "print(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection & loop RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag: 0\n",
      "R-squared scores: 0.6352\n",
      "lag: 1\n",
      "R-squared scores: 0.5946\n",
      "lag: 2\n",
      "R-squared scores: 0.5967\n",
      "lag: 3\n",
      "R-squared scores: 0.6024\n",
      "lag: 4\n",
      "R-squared scores: 0.5969\n",
      "lag: 5\n",
      "R-squared scores: 0.5989\n",
      "lag: 6\n",
      "R-squared scores: 0.6035\n",
      "lag: 7\n",
      "R-squared scores: 0.6056\n",
      "lag: 8\n",
      "R-squared scores: 0.6086\n",
      "lag: 9\n",
      "R-squared scores: 0.6113\n",
      "lag: 10\n",
      "R-squared scores: 0.6136\n",
      "lag: 11\n",
      "R-squared scores: 0.613\n",
      "lag: 12\n",
      "R-squared scores: 0.6073\n",
      "lag: 13\n",
      "R-squared scores: 0.6032\n",
      "lag: 14\n",
      "R-squared scores: 0.6026\n",
      "lag: 15\n",
      "R-squared scores: 0.6021\n",
      "lag: 16\n",
      "R-squared scores: 0.6011\n",
      "lag: 17\n",
      "R-squared scores: 0.5972\n",
      "lag: 18\n",
      "R-squared scores: 0.597\n",
      "lag: 19\n",
      "R-squared scores: 0.5957\n",
      "lag: 20\n",
      "R-squared scores: 0.5967\n",
      "lag: 21\n",
      "R-squared scores: 0.5957\n",
      "lag: 22\n",
      "R-squared scores: 0.5966\n",
      "lag: 23\n",
      "R-squared scores: 0.5975\n",
      "lag: 24\n",
      "R-squared scores: 0.5991\n",
      "lag: 25\n",
      "R-squared scores: 0.599\n",
      "lag: 26\n",
      "R-squared scores: 0.5993\n",
      "lag: 27\n",
      "R-squared scores: 0.5987\n",
      "lag: 28\n",
      "R-squared scores: 0.5984\n",
      "lag: 29\n",
      "R-squared scores: 0.5968\n",
      "lag: 30\n",
      "R-squared scores: 0.597\n",
      "lag: 31\n",
      "R-squared scores: 0.5996\n",
      "lag: 32\n",
      "R-squared scores: 0.6019\n",
      "lag: 33\n",
      "R-squared scores: 0.6021\n",
      "lag: 34\n",
      "R-squared scores: 0.6015\n",
      "lag: 35\n",
      "R-squared scores: 0.6007\n",
      "lag: 36\n",
      "R-squared scores: 0.6005\n",
      "lag: 37\n",
      "R-squared scores: 0.6011\n",
      "lag: 38\n",
      "R-squared scores: 0.6014\n",
      "lag: 39\n",
      "R-squared scores: 0.6017\n"
     ]
    }
   ],
   "source": [
    "index_list = [\n",
    "              'chlor_a','nflh', 'Rrs_678',\n",
    "              'water_temp_surface',             ## highly correlated with `sst`\n",
    "              'water_temp_dif',\n",
    "              'wind_speed',                     ## not included\n",
    "              'doy_img', \n",
    "              'velocityu_dif',                  ## highly correlated with velocityu_surface\n",
    "              'salinity_surface',\n",
    "              'salinity_dif',         \n",
    "              'Depth',                          ## highly correlated with `bathymetry`\n",
    "              \n",
    "              ## removed variables -----------------\n",
    "              'bathymetry',\n",
    "              \"velocityv_surface\", \n",
    "              \"velocityv_dif\",                 ## highly correlated with velocityv_surface\n",
    "              'velocityu_surface',             ## not included\n",
    "              'Rrs_667',                       ## highly correlated with Rrs_678\n",
    "              'Rrs_645',                       ## not included by Zilong\n",
    "              'Rrs_443',                       ## highly correlated with Rrs_412 and Rrs_469\n",
    "              'Rrs_412', 'Rrs_469',            ## not included by Zilong\n",
    "              'Rrs_488',                       ## highly correlated with Rrs_412 and Rrs_443; ## not included by Zilong\n",
    "              'Rrs_555', 'Rrs_547', 'Rrs_531', ## lowerest importance; Rrs_555 and Rrs_547 are highly correlated\n",
    "              'poc',                           ## not included by Zilong\n",
    "              'sst',                           ## not included by Zilong\n",
    "              ## -----------------------------------\n",
    "              'lon','lat']\n",
    "\n",
    "\n",
    "path_data = path_root + '\\\\data\\\\from_gee\\\\rs_do_sample\\\\rs_do_sample_lagByDay\\\\'\n",
    "\n",
    "\n",
    "## to fit the model by year, or use all data as a whole\n",
    "# by_year  = 'yes'\n",
    "by_year  = 'no'\n",
    "\n",
    "## data test finds that only using data from 'SEAMAP' can achieve better accuracy\n",
    "subset_SEAMAP = 'yes'; subset_SEAMAP_lab = 'subset_SEAMAP'\n",
    "# subset_SEAMAP = 'no';  subset_SEAMAP_lab = ''\n",
    "\n",
    "\n",
    "## create empty list in order to collect results from each run\n",
    "lag_list = [];\n",
    "yr_list  = [];\n",
    "\n",
    "random_i = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "r2_list = []\n",
    "\n",
    "zong_r2_list  = []\n",
    "zong_mae_list = []\n",
    "zong_mse_list = []\n",
    "\n",
    " \n",
    "importance_df = pd.DataFrame()\n",
    "for lag in range(0,81): # (0, 81) for initial test; (31,32) for the best lag\n",
    "        print('lag: ' + str(lag))\n",
    "        samples = pd.read_excel(path_data + 'RS_do_bottom_sample_2000_2019_byday_' + str(lag) + 'dayBefore.xlsx');\n",
    "\n",
    "        for ite in index_list:  \n",
    "            # 删除空值\n",
    "            samples = samples[samples[ite].notna()]\n",
    "        obj_num = []\n",
    "        \n",
    "        for yy in range(2003,2004): ## if by_year  = 'no', this line won't be used. \n",
    "            \n",
    "            if by_year == 'yes':\n",
    "                yr = yy\n",
    "            else:\n",
    "                yr = 1\n",
    "  \n",
    "            ## data test finds that only using data from 'SEAMAP' can achieve better accuracy\n",
    "            if subset_SEAMAP == 'yes':\n",
    "                samples2 = samples[samples['Source'] == 'SEAMAP'];\n",
    "            else:\n",
    "                samples2 = samples;\n",
    "            \n",
    "            X = samples2[index_list]\n",
    "            Y = samples2[['DO']]\n",
    "            # mae_list = []\n",
    "            # mse_list = []\n",
    "            # r2_list = []\n",
    "            for i in range(10):   #循环10次验证精度\n",
    "                X_train ,  X_test ,  y_train ,  y_test  =  train_test_split( X , Y ,  test_size  = 0.3)\n",
    "                regr = RandomForestRegressor() #参数默认\n",
    "                regr.fit(X_train, y_train.values.ravel())\n",
    "                warnings.filterwarnings('ignore')\n",
    "                predictions = regr.predict(X_test)\n",
    "                result = X_test\n",
    "                result['y_test'] = y_test\n",
    "                result['prediction'] = predictions.tolist()\n",
    "                \n",
    "                ## save the result\n",
    "                if lag in [31, 32]:\n",
    "                    file_name = path_root + '\\\\data\\\\results_RF\\\\rf_prediction_lag_' + str(lag) + 'loop' + str(i) + '.csv'\n",
    "                    result.to_csv(file_name, index=False)\n",
    "                \n",
    "                mae = mean_absolute_error(y_test.values.ravel(), predictions)\n",
    "                mse = mean_squared_error(y_test.values.ravel(), predictions)\n",
    "                r2 = r2_score(y_test.values.ravel(), predictions)\n",
    "                # print(r2)\n",
    "                mse_list.append(mse)\n",
    "                mae_list.append(mae)\n",
    "                r2_list.append(r2)\n",
    "                lag_list.append(lag)\n",
    "                yr_list.append(yr)\n",
    "                random_i.append(i)\n",
    "                \n",
    "                ## to get the variable importance score\n",
    "                import_list = []\n",
    "                score_list  = []\n",
    "                \n",
    "                characteristics = X.columns\n",
    "                importances = list(regr.feature_importances_)\n",
    "                characteristics_importances = [(characteristic, round(importance, 2)) for characteristic, importance in zip(characteristics, importances)]\n",
    "                characteristics_importances = sorted(characteristics_importances, key = lambda x: x[1], reverse = True)\n",
    "                for pair in characteristics_importances:\n",
    "                    import_list.append(pair[0])\n",
    "                    score_list.append(pair[1])\n",
    "                # [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in characteristics_importances]; #打印变量重要性\n",
    "                imp_dict = {'year': yr,\n",
    "                            'lag': lag,\n",
    "                            'random_i': i,\n",
    "                            'var': import_list,\n",
    "                            'score': score_list}\n",
    "                imp_df = pd.DataFrame(imp_dict)\n",
    "\n",
    "                ##  Starting with pandas version 1.4.0, \n",
    "                ##  the append method is deprecated and will be removed in a future release. \n",
    "                ##  Instead, you should use the concat() function to combine DataFrames\n",
    "                # importance_df = importance_df.append(imp_df, ignore_index=True) \n",
    "                importance_df = pd.concat([importance_df, imp_df], ignore_index=True)\n",
    "\n",
    "\n",
    "            ## calculate the mean of 20 models\n",
    "            # print('Mean Absolute Error:',round(mean(mae_list),2))\n",
    "            # print('Mean Squared Error:',round(mean(mse_list),2))\n",
    "            print('R-squared scores:', round(mean(r2_list),4))\n",
    "            # zong_r2_list.append(round(mean(r2_list),4))\n",
    "            # zong_mae_list.append(round(mean(mae_list), 4))\n",
    "            # zong_mse_list.append(round(mean(mse_list), 4))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save MAE and importance data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\Shared drives\\gdrive\\_paper\\_phd_dissertation\\ch4_dead_zone\\Dead_Zone_telecoupling\\data\\results_RF\\rf_r2_mse_mae_by_yearNO_28vars_subset_SEAMAP.csv\n",
      "g:\\Shared drives\\gdrive\\_paper\\_phd_dissertation\\ch4_dead_zone\\Dead_Zone_telecoupling\\data\\results_RF\\rf_importance_by_yearNO_28vars_subset_SEAMAP.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(zong_r2_list)\n",
    "# print(zong_mae_list)\n",
    "# print(zong_mse_list)\n",
    "\n",
    "# print(r2_list)\n",
    "# print(lag_list)\n",
    "# print(score_list)\n",
    "# print(characteristics_importances)\n",
    "\n",
    "# put the raw data into a dictionary of lists, and then convert it to a dataframe, then csv\n",
    "dict = {'year': yr_list,\n",
    "        'lag': lag_list,\n",
    "        'random_i': random_i,\n",
    "        'r2':  r2_list,\n",
    "        'mae': mae_list,\n",
    "        'mse': mse_list}\n",
    "# creating a dataframe from list\n",
    "df = pd.DataFrame(dict)\n",
    "\n",
    "file_name = path_root + '\\\\data\\\\results_RF\\\\rf_r2_mse_mae_' + 'by_year' + by_year.upper() + '_' + str(len(index_list)) + 'vars_' + subset_SEAMAP_lab +'.csv' ; print(file_name)\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "file_name = path_root + '\\\\data\\\\results_RF\\\\rf_importance_' + 'by_year' + by_year.upper() + '_' + str(len(index_list)) + 'vars_' + subset_SEAMAP_lab +'.csv' ; print(file_name)\n",
    "importance_df.to_csv(file_name, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
