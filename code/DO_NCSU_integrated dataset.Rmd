---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Dirs and Packages
```{r}
# To clear your environment 
remove(list = ls())

# Set work dir ----------------------------------------------------------
path <- rstudioapi::getSourceEditorContext()$path
dir  <- dirname(rstudioapi::getSourceEditorContext()$path); dir
setwd(dir)
setwd('./DO_Integrated_Matli_NCSU')
getwd()
today <- format(Sys.time(), "%Y%m%d"); print(today)
library(readxl)
library(XLConnect)
library(xlsx)
library(tidyverse)
library(dplyr)
library(proj4)

library(sf)
library(maps)
library(mapdata)

library(RColorBrewer)
library(viridis)
# devtools::install_github("jaredhuling/jcolors")
library(jcolors)


## keep more decimals 
options(digits = 15)
options(pillar.sigfig = 15)
```


*Data source*: 
  Rohith Matli, Center for Geospatial Analytics, North Carolina State University; vmatli@ncsu.edu
  
*Reference*:  
  Matli, Venkata Rohith Reddy, Shiqi Fang, Joseph Guinness, Nancy N Rabalais, J. Kevin Craig, and Daniel R. Obenour. “A Space-Time Geostatistical Assessment of Hypoxia in the Northern Gulf of Mexico.” Environmental Science & Technology, September 28, 2018. https://doi.org/10.1021/acs.est.8b03474.
  Matli, Venkata Rohith Reddy, Arnaud Laurent, Katja Fennel, Kevin Craig, Jacob Krause, and Daniel R. Obenour. “Fusion-Based Hypoxia Estimates: Combining Geostatistical and Mechanistic Models of Dissolved Oxygen Variability.” Environmental Science & Technology, September 3, 2020. https://doi.org/10.1021/acs.est.0c03655


# Data read in

*SBToML* - Observations from *Seabird* Instrument - Rosette mounted sensor. Don’t Usually go all the way to the bottom of the ocean but have complete profile information

*HLToML* - Observations from *Handhold DO probes* - Mostly used by *LUMCON*. Usually go all the way to the bottom of the ocean. But don’t have complete water column profiles like rosette mounted sensors.

*StatoML* - Summary of all Locations and dates of observations with the Keys (YEID) linking to the observations in SBToML and HLToMl data files.
	- E&N in UTM Zone 15N (km) . Note if transformations are used Zone 15N is usually read in m, so apply a correction factor
	- Depth in m. Obtained from NOAA raster. Refer Obenour' 2015 or Matli' 2018
	- ShelfW: Whether the cruise is shelfwide or focuses of certain sections of the shelf
	- Source: Organization collecting data
	- Corr: Addition correction factor applied to account for bias in sampling
*Note*
  - UMCES observations were not binned properly when exported from CTD software, resulting in fine scale profile information. 
	- Some observations have observations from both lowering and raising of the rosette
	- Some observations have readings from both Rosette and handhold probes. Mostly in the case of data collected by LUMCON (Nancy's Program)
	- LDWF have limited spatial range. 


```{r Read data, eval=FALSE, include=FALSE}

### Read in data ------------------------------------------------------------
getwd()
list.files(path = './data', pattern = '.xlsx$', full.names = T)

df1.xlsx <- list.files(path = './data', pattern = '^Summary.xlsx', full.names = T); df1.xlsx
df2.xlsx <- list.files(path = './data', pattern = '^RawData.xlsx', full.names = T); df2.xlsx

df11 <- read_excel(path = df1.xlsx, sheet = 1, col_names = T) # Summary
df21 <- read_excel(path = df2.xlsx, sheet = 1, col_names = T) # SBToML
df22 <- read_excel(path = df2.xlsx, sheet = 2, col_names = T) # HLToML
df23 <- read_excel(path = df2.xlsx, sheet = 3, col_names = T) # StaToML

# df <- read.xlsx(file = df.xlsx, sheetName = "StaToML", as.data.frame = T)
# library("openxlsx")
# df <- read.xlsx(xlsxFile = df.xlsx, sheet = 'StaToML', colNames = TRUE)


## the summary data file
dfsummary <- df11

## observation data 1
sb <- df21 %>% as.data.frame() ## Observations from Seabird Instrument. Don’t Usually go all the way to the bottom of the ocean but have complete profile information

## observation data 1
hh <- df22 %>% as.data.frame() ## Observations from Handheld DO probes - Mostly used by LUMCON. Usually go all the way to the bottom of the ocean. But don’t have complete water column profiles like rosette mounted sensors

## spatial and date info for data 1 and data 2
st <- df23 %>% as.data.frame() ## Summary of all Locations and dates of observations with the Keys (YEID) linking to the observations in SBToML and HLToMl data files


head(sb)
head(hh)
head(st, 3)


### clean data -----------------------------------------------------------------
### fix - ID should be int
sb$EventID <- round(sb$EventID) 
hh$EventID <- round(hh$EventID)
# unique(st$EventID)
st$EventID <- round(st$EventID)
st$OEID    <- round(st$OEID)

### fix - YEID should be string
sb$YEID <- as.character(sb$YEID)
hh$YEID <- as.character(hh$YEID)
st$YEID <- as.character(st$YEID)

### fix - name
names(sb)
names(sb) <- gsub(pattern = '%', replacement = '', x = names(sb), ignore.case = T)
names(hh) <- gsub(pattern = '%', replacement = '', x = names(hh), ignore.case = T)
names(st) <- gsub(pattern = '%', replacement = '', x = names(st), ignore.case = T)


### Save as R data ------------------------------------------------------------
# save(dfsummary,  file = "./data/Summary.RData")
# save(sb, hh, st, file = "./data/RawData.RData")

```



# Data cleaning 
```{r}

### To load the data again  ---------------------------------------------------
getwd()
load("./data/Summary.RData")
load("./data/RawData.RData")


### convert UTM 15 N to lat/lon ----------------------------------------------
sp <- st %>%
  drop_na() %>%
  mutate(E1000 = E*1000, N1000 = N*1000)

library(proj4)
proj4string <- "+proj=utm +zone=15 +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs "

## lat and lon data
xy <- data.frame(x=sp$E1000, y=sp$N1000)

## Transformed data
pj <- project(xy = xy, proj = proj4string, inverse = T, degrees = T)
sp <- data.frame(sp, lat = pj$y, lon = pj$x) 
head(sp)

## change levels of the data sources
unique(sp$Source)
mylevel <- c("LUMCON", "TAMU", "UMCES", "SEAMAP", "EPA", "NECOP", "LDWF", "latx")
sp$Source <- factor(sp$Source, levels = mylevel)
levels(sp$Source)
## reverse the levels
sp$Source <- fct_rev(sp$Source)

## remove some columns
head(sp)
str(sp)
sp0 <- sp %>% dplyr::select(-c("E", "N", "E1000", "N1000")) 


### check this data 
library(stringr)
str_pad(string = 1, width = 3, pad = "0")
sp_check <- sp0 %>%           
  dplyr::mutate(
    EventID_int = as.integer(EventID),
    check       = EventID_int - OEID,
    YEID2       = paste0(Year, '.', str_pad(string = EventID_int, width = 3, pad = "0"))
    ) %>%
  arrange(YEID2, YEID) %>%
  dplyr::select(YEID, YEID2, Year, OEID, EventID, EventID_int, check) %>%
  dplyr::filter(check != 0)
# unique(sp_check$check)
## ??? OEID VS. EventID ???
## --> Response from Rohith Reddy Matli: OEID is the index Id. I had to reorder some of them due to addition of data from other programs. It was a bit confusing initially, but I have improved it over time. 
## so, basically, OEID (updated EventID) is the new index, acting as the same role of EventID in the two data. 

sp1 <- sp0 %>%
  dplyr::select(-EventID)


### merge data and spatial info ------------------------------------------------
names(sb)
names(hh)
names(sp1)

str(sb)
str(hh)
str(sp1)

sp1$YEID[duplicated(sp1$YEID)] ## no duplicated rows
length(unique(sp1$YEID))       ## no duplicated rows, confirm the YEID is unique

## note: YEID = Year + EventID
sb.sp <- merge(x = sb, y = sp1, by = 'YEID', all.x = T)
hh.sp <- merge(x = hh, y = sp1, by = 'YEID', all.x = T)
head(sb.sp, 3)
head(hh.sp, 3)



### check merged data ---------------------------------------------------------
#### 1. for sb's DO   ------------------
sb.sp.na  <- sb.sp %>% filter(is.na(DO))   ## 10 NA 
##--- after checking back to the xlsx, these rows with NA should be 0
sb.sp <- sb.sp %>%
  # mutate(DO = replace(DO, is.na(DO), 0))
 dplyr::mutate(DO = if_else(is.na(DO), 0, DO))

#### 2. for sb's source
sb.sp.na_source  <- sb.sp %>% filter(is.na(Source))   ## 5w+ NA ??????????????????????????????????????????????????????????
## % have no source info?
percent(nrow(sb.sp.na_source)/nrow(sb.sp)) 
## which YEID?
unique(sb.sp.na_source$YEID)
sp1 %>%
  filter(YEID == '2005.614')

sb.sp.yr  <- sb.sp %>% filter_(~Year.x    != Year.y)  ## none
# sb.sp.ev1 <- sb.sp %>% filter_(~EventID.x != EventID.y)
# sb.sp.ev2 <- sb.sp %>% filter_(~EventID.x != OEID)    ## good - none
sb.sp.ev  <- sb.sp %>% filter_(~EventID   != OEID)    ## good - none
sp.ev     <- sp    %>% filter_(~EventID   != OEID)    ## re-indexed EventID to OEID
sb.sp.dep <- sb.sp %>% filter_(~Depth.x   != Depth.y) ## many are different (> 735810) ??????????????????????????????????


#### for hh ------------------------------
hh.sp.na  <- hh.sp %>% filter(is.na(DO))              ## good - 0 NA
hh.sp.yr  <- hh.sp %>% filter_(~Year.x    != Year.y)  ## good
hh.sp.ev2 <- hh.sp %>% filter_(~EventID   != OEID)    ## good
hh.sp.dep <- hh.sp %>% filter_(~Depth.x   != Depth.y) ## many are different (35631)



### update the dataframe ------------------------------------------------------
sb.sp.update <- sb.sp %>% 
  dplyr::select(-grep('.y', names(.), ignore.case = T)) %>%      ## remove such cols
  rename_at(vars(contains('.x')), funs(sub('.x', '', .)))        ## rename such cols

hh.sp.update <- hh.sp %>% 
  dplyr::select(-grep('.y', names(.), ignore.case = T)) %>%      ## remove such cols
  rename_at(vars(contains('.x')), funs(sub('.x', '', .)))        ## rename such cols

names(sb.sp.update)
names(hh.sp.update)




### bind two data together ------------------------------------------------------
df <- rbind(
  cbind(sb.sp.update, Instrument = 'SB'),
  cbind(hh.sp.update, Instrument = 'HH')
) %>%
  arrange(Year, YEID, Source)


df.na.source <- df %>% filter(is.na(Source))     ## many ?????????????????????????????????????????????????????????????????
df.na.DO     <- df %>% filter(is.na(DO))         ## none


## data description
print(paste0('Total sample points:   ', nrow(df)))
print(paste0('Total sample location: ', nrow(sp)))

### save to csv ---------------------------------------------------------------
today <- format(Sys.time(), "%Y%m%d"); print(today)
fnamerdt <- paste0("./data/", 'DO_Integrated_Matli_NCSU_cleaned_', today, '.RData'); fnamerdt
fnamecsv <- paste0("./data/", 'DO_Integrated_Matli_NCSU_cleaned_', today, '.csv');   fnamecsv
save(df, file = fnamerdt)
write.csv(df, file = fnamecsv)

```






```{r check data in 2015}

str(df)
unique(df$Source)

df.2015 <- df %>%
  filter(Source == 'SEAMAP' &
           Year == 2015)

length(unique(df.2015$YEID)) ## 2015: 94

df.2014 <- df %>%
  dplyr::filter(Source == 'LUMCON' &
           Year == 2014)
length(unique(as.character(df.2014$YEID))) ## 2014: 86

```



```{r check data from lsu}
lsu <- sb.sp %>%
  dplyr::filter(Source == 'LUMCON' & Year >= 2000) 

str(lsu)
lsu %>% ggplot() + 
  geom_bar(aes(x=factor(Year))) + 
  theme_bw()
```




# Map of samples by year & by source
```{r}
library(sf)
library(maps)
library(mapdata)

bg_transparent <- +
  theme(
    panel.background = element_rect(fill = "transparent"),             # bg of the panel
    plot.background  = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"),    # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent") # get rid of legend panel bg
  )

shp <- map_data('usa') ## world
head(shp)




## 1. sample number stack bar by year ------------------------------------------------------------
size_key = 0.4
st %>% ggplot() + 
  geom_bar(aes(x=Year, fill = Source), position = "stack") + 
  ylab('Sampling sites') +
  scale_x_continuous(breaks = seq(1985, 2017, by = 2))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        # legend.key.size   = unit(size_key, "cm"),
        legend.key.height = unit(size_key, "cm"),
        legend.key.width  = unit(size_key, "cm")) +
  # guides(shape = guide_legend(override.aes = list(size = size_key)), ## change the legend key size
  #        color = guide_legend(override.aes = list(size = size_key)),
  #        fill  = guide_legend(override.aes = list(size = size_key))) +
  theme(legend.position = c(0.1, 0.8)) 

pname <- paste0('./figures/', 'total_count_sample_bySources1.png'); pname
ggsave(filename = pname, plot = last_plot(),  bg = "transparent",
       width = 6, height = 6, units = 'in', dpi = 300)




## 2. sample distribution facet by year ----------------------------------------------------------
ggplot() + 
  # geom_polygon(data = shp, aes(x=long, y = lat, group = group), fill = 'gray90') + 
  # scale_fill_viridis_d(option = "viridis", alpha = 0.6)+
  geom_point(data = st,  # use geom_point, instead of geom_sf
             aes(x = lng, y = lat, color = Source), alpha = 0.9, size = 1.5) +
  # scale_color_brewer(palette = "Dark2") +
  facet_wrap(.~Year)+
  theme_bw() +
  guides(color = guide_legend(override.aes = list(size=4))) ## enlarge the key size

getwd()
pname <- paste0('./figures/', 'map_samp_by year1', '.png'); pname
ggsave(filename = pname, plot = last_plot(), 
       width = 14, height = 8, units = 'in', dpi = 300)




## 3. sample distribution facet by data source ---------------------------------------------------
ggplot() + 
  geom_point(data = st,  # use geom_point, instead of geom_sf
             aes(x = lng, y = lat, color = Year), alpha = 0.5) +
  facet_wrap(.~Source)+
  # scale_color_gradient2(low = 'blue', mid = 'green', high = 'red', midpoint = 2000)+
  scale_color_viridis(option = "D") +
  theme_bw()
pname <- paste0('./figures/', 'map_samp_by source1', '.png'); pname
ggsave(filename = pname, plot = last_plot(), 
       width = 14, height = 8, units = 'in', dpi = 300)



```